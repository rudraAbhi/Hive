create database hive_class_b1;
use hive_class_b1;

create table department_data                                                                                                            
    > (                                                                                                                                       
    > dept_id int,                                                                                                                            
    > dept_name string,                                                                                                                       
    > manager_id int,                                                                                                                         
    > salary int)                                                                                                                             
    > row format delimited                                                                                                                    
    > fields terminated by ','; 
    
describe department_data;

describe formatted department_data;--[for detailed table info]

# For data load from local
load data local inpath 'file:///tmp/hive_class/depart_data.csv' into table department_data; 

# Display column name
set hive.cli.print.header = true;

# Load data from hdfs location
load data inpath '/tmp/hive_data_class_2/' into table department_data_from_hdfs;


# Create external table 

create external table department_data_external                                                                                          
    > (                                                                                                                                       
    > dept_id int,                                                                                                                            
    > dept_name string,                                                                                                                       
    > manager_id int,                                                                                                                         
    > salary int                                                                                                                              
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > location '/tmp/hive_data_class_2/'; 
    
    
    
# work with Array data types

create table employee                                                                                                                   
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > name string,                                                                                                                            
    > skills array<string>                                                                                                                    
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > collection items terminated by ':';                                                                                                     

load data local inpath 'file:///tmp/hive_class/array_data.csv' into table employee; 


# Get element by index in hive array data type

select id, name, skills[0] as prime_skill from employee;

select                                                                                                                                  
    > id,                                                                                                                                     
    > name,                                                                                                                                   
    > size(skills) as size_of_each_array,                                                                                                     
    > array_contains(skills,"HADOOP") as knows_hadoop,                                                                                        
    > sort_array(skills) as sorted_array                                                                                                                     
    > from employee; 
    
    
# table for map data

create table employee_map_data                                                                                                          
    > (                                                                                                                                       
    > id int,                                                                                                                                 
    > name string,                                                                                                                            
    > details map<string,string>                                                                                                              
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','                                                                                                                
    > collection items terminated by '|'                                                                                                      
    > map keys terminated by ':';
    
 load data local inpath 'file:///tmp/hive_class/map_data.csv' into table employee_map_data;
 
 select                                                                                                                                  
    > id,                                                                                                                                     
    > name,                                                                                                                                   
    > details["gender"] as employee_gender                                                                                                    
    > from employee_map_data; 
 
 # map functions
 select                                                                                                                                  
    > id,                                                                                                                                     
    > name,                                                                                                                                   
    > details,                                                                                                                                
    > size(details) as size_of_each_map,                                                                                                      
    > map_keys(details) as distinct_map_keys,                                                                                                 
    > map_values(details) as distinct_map_values                                                                                              
    > from employee_map_data; 



# Assignment Dataset

https://www.kaggle.com/datasets/imdevskp/corona-virus-report


-----------------------Class Hive 3-----Date: 10sept-------------------------------------------

# haddop fs -ls /
#hadoop fs -ls /user/hive/warehouse/practice*[get all list of db's]
# hadoop fs -ls /user/hive/warehouse/hive_class_b1.db/employee(Array_data csv comes here)
# vim sales_data.csv  {to save and exit - esc -> :wq} 
# cat sales-data.csv {to watch the content of it}
# cp sales_data.csv destinbationLocationDirectory

# first load data as csv

create table sales_data_v2                                                                                                              
    > (                                                                                                                                       
    > p_type string,                                                                                                                          
    > total_sales int                                                                                                                         
    > )                                                                                                                                       
    > row format delimited                                                                                                                    
    > fields terminated by ','; 
    
    
load data local inpath 'file:///tmp/hive_class/sales_data_raw.csv' into table sales_data_v2; 

# command to create identical table
create table sales_data_v2_bkup as select * from sales_data_v2;

# describe command for a table
describe extended sales_data_v2;


# create a table which will store data in parquet

create table sales_data_pq_final                                                                                                        
    > (                                                                                                                                       
    > product_type string,                                                                                                                    
    > total_sales int                                                                                                                         
    > )                                                                                                                                       
    > stored as parquet;  
    
# load data in parquet file
from sales_data_v2 insert overwrite table sales_data_pq_final select *;


---- Execute hdfs command in hive---0-
hdfs dfs -ls/....
(Just remove hdfs from here )- dfs -ls /user/...


Note- To see serialised data from hdfs then go to that pertciular location and see. Use cat for watchning the content.

---------------------------hive class 4 Practical---------------------------------------------------------------------------

Internal table = managed table
Serde - serialisation and deserialisation

Input format of pq table- Parquet serde
Output format of ""- Parquet serde
serde library of ''- Parquet hive serde

HQL get converted to --- mapReduce task in backednd

complete process of serialisation and deserialisation in hadoop------
            
            [HDFS] -----------------------------> [MAP] --------|||||| --------[REDUCE]--------------------- -------------------------> [HDFS]
            {have deseria;ised data}          {use serde library to             {use serde library to process pq table to       { create a seria;ised table data }
                                                 process parquet table data      reduce in backend }
                                                 and map it }
                                                 
 NOTE 1. If the incoming raw data in any specific format (Not in simple ',' delimjted then we use a library for it)(Library can be found on web)
 data sample-
    name, location
    Amit, "Bangalore, India"
    Rahul,"Ranchi,India"
    ravi,"Jammu,India"
    
 ex- to craete a table which takes data from a csv file in a speficic format----\
    # create table csv_table (
    name string, location string
    ) 
    row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
    with serdeproperties ("separatorChar" =",",                                                      //// for separator
            "quoteChar"="/"",                                                                           
            "escapeChar"="//")                                                                          // to escape a char
            stored as textfile                                                                           /// storing as textfile
            tblproperties ("skip.header.line.count" ="1")                                                ///line 1 is header , so added this 
            
